{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk import sent_tokenize\n",
    "from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import time\n",
    "import copy\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"Terms_Key_words.csv\"\n",
    "users = [\n",
    "    [\"terms\", \"key_words\"],\n",
    "]\n",
    "with open(PATH, \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>accessiblecomputing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>anarchism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>afghanistanhistory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>afghanistangeography</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>afghanistanpeople</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                  Word\n",
       "0      0   accessiblecomputing\n",
       "1      1             anarchism\n",
       "2      2    afghanistanhistory\n",
       "3      3  afghanistangeography\n",
       "4      4     afghanistanpeople"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data = pd.read_csv('Index_words_eng.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = [0, 250]\n",
    "n = list(data.Index)[INDEX[0] : INDEX[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_corpus(index: list or tuple =[0,10], corpus:list = []) -> None:\n",
    "    '''Чтение файлов с текстами собирая их в массив для последущих манипуляций'''\n",
    "    index = list(data.Index)[index[0]: index[1]]\n",
    "    for i in index:\n",
    "        with open(f\"dataset_eng/{i}.txt\", 'r') as fl:\n",
    "            text = fl.read().lower()\n",
    "            text = re.sub(r'/(style=\")([a-zA-Z0-9:;\\.\\s\\(\\)\\-\\,]*)(\")/gi', '', text)\n",
    "            text = re.sub(r'ref|url|link|title|aa|url', '', text)\n",
    "            text = re.sub(r\"title\", '', text)\n",
    "            text = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»\"\"‘’]))', '', text)\n",
    "            text = re.sub(r\"[^a-zA-Z\\.]+\", \" \", text)\n",
    "       \n",
    "        corpus.append(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "def key_words(corpus) -> list:\n",
    "    \"\"\"Выделение ключевых слов с использованием BERT\"\"\"\n",
    "    res = []\n",
    "    for text in corpus:\n",
    "        try:            \n",
    "            n1 = model.extract_keywords(text, keyphrase_length=1, use_maxsum=True)\n",
    "            n2 = model.extract_keywords(text, keyphrase_length=2, use_maxsum=True)\n",
    "#             n3 = model.extract_keywords(text, keyphrase_length=3, use_maxsum=True)\n",
    "#             n4 = model.extract_keywords(text, keyphrase_length=4, use_maxsum=True)\n",
    "            keywords = n1 + n2 # + n3 + n4 \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            info = e\n",
    "            keywords = ['None' ]\n",
    "        res.append(keywords)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataFrame(data:pd.DataFrame, result, index) -> pd.DataFrame:\n",
    "    return pd.DataFrame({i:j for i,j in zip(list(data.loc[index[0]:index[1], 'Word']), result)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_tfidf(corpus, res, n= 4) -> None:\n",
    "    \"\"\"ИСпользование n-Grams & TF-IDf\"\"\"\n",
    "    for index_text, text in enumerate(corpus):\n",
    "        size = (1,1)\n",
    "        text = re.sub(r\"shell|redirect|category\", ' ', text)\n",
    "        data = []\n",
    "        \n",
    "        \n",
    "        x2_variaons = {i : TfidfVectorizer(ngram_range = (i, i)).fit_transform(corpus)  for i in range(2, n+1)}\n",
    "        \n",
    "        \n",
    "        for i in range(2, n + 1):\n",
    "            size = (i, i)\n",
    "            vectorizer = CountVectorizer(ngram_range =size) \n",
    "            try:\n",
    "                X1 = vectorizer.fit_transform(sent_tokenize(text))  \n",
    "                features = (vectorizer.get_feature_names()) \n",
    "\n",
    "                # Applying TFIDF \n",
    "                # You can still get n-grams here \n",
    "#                 vectorizer = TfidfVectorizer(ngram_range = size) \n",
    "#                 X2 = vectorizer.fit_transform(corpus)\n",
    "                X2 = x2_variaons[i]\n",
    "#                 scores = (X2.toarray()) \n",
    "\n",
    "                # Getting top ranking features \n",
    "                sums = X2.sum(axis = 0) \n",
    "\n",
    "                for col, term in enumerate(features): \n",
    "                    data.append( (term, sums[0, col] )) \n",
    "            except Exception as e:\n",
    "                info = e\n",
    "                #print(e)\n",
    "        ranking = pd.DataFrame(data, columns = ['term', 'rank']) \n",
    "        words = (ranking.sort_values('rank', ascending = False)) \n",
    "        [res[index_text].append(_) for _ in list(words.head().term.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_of_pillars(example_output) -> None:\n",
    "    \"\"\"Выравнивание всех колонок по длине для построение датафрейма\"\"\"\n",
    "    for index, i in enumerate(example_output):\n",
    "        if len(i) < max([len(i) for i in example_output]):\n",
    "            while len(i)< max([len(i) for i in example_output]):\n",
    "                example_output[index].append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_crossing(arg:list) -> list:\n",
    "    \"\"\"Группировка ключевых n-Gramm тайтла по общему слову\"\"\"\n",
    "    tmp = []\n",
    "    for i in arg:\n",
    "        test = re.sub(r'shell|category|redirect|camelcase', '', i)\n",
    "        if len(test) > 0:\n",
    "            if len(test.split() ) == 1:\n",
    "                tmp.append(re.sub(r'\\s+', '', test))\n",
    "            else :\n",
    "                tmp.append(test)\n",
    "    tmp = list(set(tmp))\n",
    "    result_new = []\n",
    "    for index, el in enumerate(tmp):\n",
    "        test = []\n",
    "        for index_, el_ in enumerate(tmp):\n",
    "            if el_ != 'None' and len(set(el.split()) & set(el_.split())) > 0 :\n",
    "                test.append(el_)\n",
    "                tmp[index_] = 'None'\n",
    "        if len(test) > 0 :\n",
    "            if len(test) > 1 :\n",
    "                result_new.append(set(test))\n",
    "            else:\n",
    "                result_new.append(test[0])\n",
    "    return result_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognazie_spam(array_words:list) -> list:\n",
    "    \"\"\"Поиск неевалидной информации(которая не относится к терминам)\"\"\"\n",
    "    result = []\n",
    "    for  el in array_words:\n",
    "        flag = True\n",
    "        if type(el) == set:\n",
    "            text = \" \".join(el)\n",
    "            doc = nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.3257277011871338 append_corpus ---\n",
      "--- 148.37134194374084 corpus_with_key_words ---\n",
      "--- 141.02721977233887 n_gram_tfidf ---\n",
      "--- 0.7800981998443604 recognazie_spam ---\n",
      "0 100\n",
      "--- 0.9061279296875 append_corpus ---\n",
      "--- 334.41275119781494 corpus_with_key_words ---\n",
      "--- 330.59163999557495 n_gram_tfidf ---\n",
      "--- 1.28175950050354 recognazie_spam ---\n",
      "100 200\n"
     ]
    }
   ],
   "source": [
    "from_ = 0\n",
    "step = 100\n",
    "result_corpus = []\n",
    "for to in np.arange(step, len(n)+ 1, step):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    corpus = []\n",
    "    index= [from_, to]\n",
    "    \n",
    "    append_corpus(index=index, corpus=corpus)\n",
    "    \n",
    "    print(\"--- %s append_corpus ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "    corpus_with_key_words = key_words(corpus)\n",
    "    \n",
    "    print(\"--- %s corpus_with_key_words ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    n_gram_tfidf(corpus=corpus, res = corpus_with_key_words)\n",
    "    \n",
    "    print(\"--- %s n_gram_tfidf ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for el  in corpus_with_key_words:\n",
    "        result_corpus.append(recognazie_spam(group_crossing(el)))\n",
    "     \n",
    "    print(\"--- %s recognazie_spam ---\" % (time.time() - start_time))\n",
    "\n",
    "    \n",
    "    print(from_, to)\n",
    "    from_= to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output = copy.deepcopy(result_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_of_pillars(example_output=example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_DataFrame(data=data, result=example_output, index=[INDEX[0], INDEX[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accessiblecomputing</th>\n",
       "      <th>anarchism</th>\n",
       "      <th>afghanistanhistory</th>\n",
       "      <th>afghanistangeography</th>\n",
       "      <th>afghanistanpeople</th>\n",
       "      <th>afghanistancommunications</th>\n",
       "      <th>afghanistantransportations</th>\n",
       "      <th>afghanistanmilitary</th>\n",
       "      <th>afghanistantransnationalissues</th>\n",
       "      <th>assistivetechnology</th>\n",
       "      <th>...</th>\n",
       "      <th>amateur astronomy</th>\n",
       "      <th>astronomers and astrophysicists</th>\n",
       "      <th>aikido</th>\n",
       "      <th>art</th>\n",
       "      <th>albania/history</th>\n",
       "      <th>albania/transnational issues</th>\n",
       "      <th>albania/people</th>\n",
       "      <th>albania/foreign relations</th>\n",
       "      <th>agnostida</th>\n",
       "      <th>abortion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{accessibility, accessibility  unprintworthy}</td>\n",
       "      <td>liberalism</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>afghanistan</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>afghan</td>\n",
       "      <td>None</td>\n",
       "      <td>{assistive technology , assistive, technology,...</td>\n",
       "      <td>...</td>\n",
       "      <td>solar</td>\n",
       "      <td>None</td>\n",
       "      <td>major styles</td>\n",
       "      <td>july</td>\n",
       "      <td>history</td>\n",
       "      <td>{subpage, subpage nahmc foreign, subpage nahmc...</td>\n",
       "      <td>albania</td>\n",
       "      <td>{subpage, subpage nahmc foreign, subpage nahmc...</td>\n",
       "      <td>glyptagnostidae</td>\n",
       "      <td>abortionist midwife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unprintworthy</td>\n",
       "      <td>counterculture</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>ra declination dec</td>\n",
       "      <td>None</td>\n",
       "      <td>little caricatures</td>\n",
       "      <td>{cancer, cancer accessdate}</td>\n",
       "      <td>None</td>\n",
       "      <td>relations</td>\n",
       "      <td>None</td>\n",
       "      <td>relations</td>\n",
       "      <td>taxobox</td>\n",
       "      <td>condoms reported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>capitalist</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>{skygazing, skygazing skygazing, astronomy sky...</td>\n",
       "      <td>None</td>\n",
       "      <td>judo students</td>\n",
       "      <td>decisive role</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>author valent volume</td>\n",
       "      <td>cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>rough consensus</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>sungrazers</td>\n",
       "      <td>None</td>\n",
       "      <td>aikidoka</td>\n",
       "      <td>birth venus</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>sam gon</td>\n",
       "      <td>{outpatient abortion, abortion termination, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>pages date harv cite</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>asteroids</td>\n",
       "      <td>None</td>\n",
       "      <td>ky jitai</td>\n",
       "      <td>visitors viewers</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>year agnostids entombed</td>\n",
       "      <td>volume page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             accessiblecomputing             anarchism  \\\n",
       "0  {accessibility, accessibility  unprintworthy}            liberalism   \n",
       "1                                  unprintworthy        counterculture   \n",
       "2                                           None            capitalist   \n",
       "3                                           None       rough consensus   \n",
       "4                                           None  pages date harv cite   \n",
       "\n",
       "  afghanistanhistory afghanistangeography afghanistanpeople  \\\n",
       "0               None                 None       afghanistan   \n",
       "1               None                 None              None   \n",
       "2               None                 None              None   \n",
       "3               None                 None              None   \n",
       "4               None                 None              None   \n",
       "\n",
       "  afghanistancommunications afghanistantransportations afghanistanmilitary  \\\n",
       "0                      None                       None              afghan   \n",
       "1                      None                       None                None   \n",
       "2                      None                       None                None   \n",
       "3                      None                       None                None   \n",
       "4                      None                       None                None   \n",
       "\n",
       "  afghanistantransnationalissues  \\\n",
       "0                           None   \n",
       "1                           None   \n",
       "2                           None   \n",
       "3                           None   \n",
       "4                           None   \n",
       "\n",
       "                                 assistivetechnology  ...  \\\n",
       "0  {assistive technology , assistive, technology,...  ...   \n",
       "1                                               None  ...   \n",
       "2                                               None  ...   \n",
       "3                                               None  ...   \n",
       "4                                               None  ...   \n",
       "\n",
       "                                   amateur astronomy  \\\n",
       "0                                              solar   \n",
       "1                                 ra declination dec   \n",
       "2  {skygazing, skygazing skygazing, astronomy sky...   \n",
       "3                                         sungrazers   \n",
       "4                                          asteroids   \n",
       "\n",
       "  astronomers and astrophysicists              aikido  \\\n",
       "0                            None        major styles   \n",
       "1                            None  little caricatures   \n",
       "2                            None       judo students   \n",
       "3                            None            aikidoka   \n",
       "4                            None            ky jitai   \n",
       "\n",
       "                           art albania/history  \\\n",
       "0                         july         history   \n",
       "1  {cancer, cancer accessdate}            None   \n",
       "2                decisive role            None   \n",
       "3                  birth venus            None   \n",
       "4             visitors viewers            None   \n",
       "\n",
       "                        albania/transnational issues albania/people  \\\n",
       "0  {subpage, subpage nahmc foreign, subpage nahmc...        albania   \n",
       "1                                          relations           None   \n",
       "2                                               None           None   \n",
       "3                                               None           None   \n",
       "4                                               None           None   \n",
       "\n",
       "                           albania/foreign relations                agnostida  \\\n",
       "0  {subpage, subpage nahmc foreign, subpage nahmc...          glyptagnostidae   \n",
       "1                                          relations                  taxobox   \n",
       "2                                               None     author valent volume   \n",
       "3                                               None                  sam gon   \n",
       "4                                               None  year agnostids entombed   \n",
       "\n",
       "                                            abortion  \n",
       "0                                abortionist midwife  \n",
       "1                                   condoms reported  \n",
       "2                                             cancer  \n",
       "3  {outpatient abortion, abortion termination, pr...  \n",
       "4                                        volume page  \n",
       "\n",
       "[5 rows x 196 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['liberalism',\n",
       " 'counterculture',\n",
       " 'capitalist',\n",
       " 'rough consensus',\n",
       " 'pages date harv cite',\n",
       " 'century anarchists',\n",
       " 'women sexual freedom',\n",
       " 'involvement protests',\n",
       " {'economics anarchist', 'sectarianism anarchist'},\n",
       " 'postcolonialism',\n",
       " 'fascists',\n",
       " 'means production sfn']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(data : pd.DataFrame, example_output : dict) -> None:   \n",
    "    for i,j in zip(list(data.loc[INDEX[0]:INDEX[1], 'Word']), example_output):\n",
    "            if len(j) != 0 :\n",
    "                if len(j) == 1 and j[0] == 'camelcase':\n",
    "                    continue\n",
    "                with open(PATH, \"a\", newline=\"\") as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerows([[i, j]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Index_words_eng.csv')\n",
    "save(data=data, example_output=result_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}